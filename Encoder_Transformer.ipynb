{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Encoder-Transformer\n",
        "\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import zipfile\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(file_content : str, lemmatize: bool , html_parser: bool, rm_numbers: bool):\n",
        "  # with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "  # with open(file_path, \"r\") as file:\n",
        "  #   file_content = file.read().decode('utf-8').lower()\n",
        "  #content = file_content.strip().split('\\n')\n",
        "  content = file_content.lower()\n",
        "  if html_parser:\n",
        "    content = BeautifulSoup(content, \"html.parser\").text\n",
        "  if rm_numbers:\n",
        "    content = re.sub(r'\\d+', '', content)\n",
        "  content = ' '.join(content.split())\n",
        "  tokens = word_tokenize(content)\n",
        "  tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "  if lemmatize:\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "  return tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ior0jh9vYpVf",
        "outputId": "ea939da0-198e-4b43-a018-600f33012b6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBZzvnl8YJvY"
      },
      "outputs": [],
      "source": [
        "# Input Embeddings\n",
        "import numpy as np\n",
        "from gensim.downloader import load\n",
        "from scipy.sparse import vstack, csr_matrix\n",
        "\n",
        "def create_embeddings(sent_tokens, model):\n",
        "\n",
        "  num_columns = model.vector_size\n",
        "  # num_rows = 0  # No rows initially\n",
        "  word_embeddings = []\n",
        "  # Create the csr_matrix\n",
        "  # embed_matrix = csr_matrix((num_rows, num_columns))\n",
        "  for token in sent_tokens:\n",
        "    if token in model: #model.key_to_index:\n",
        "      word_embeddings.append(model[token])\n",
        "    else:\n",
        "      word_embeddings.append(np.zeros(model.vector_size))\n",
        "\n",
        "  embed_matrix = csr_matrix(word_embeddings)\n",
        "  return embed_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def positional_encodings(sent_tokens, model_size=50):\n",
        "    num_columns = model_size\n",
        "    sent_len = len(sent_tokens)\n",
        "\n",
        "    encodings = []\n",
        "\n",
        "    for pos in range(sent_len):\n",
        "        pos_encoding = np.zeros(num_columns)\n",
        "        for i in range(num_columns):\n",
        "            if i % 2 == 0:\n",
        "                pos_encoding[i] = np.sin(pos / np.power(10000, (2 * i) / num_columns))\n",
        "            else:\n",
        "                pos_encoding[i] = np.cos(pos / np.power(10000, (2 * i) / num_columns))\n",
        "        encodings.append(pos_encoding)\n",
        "\n",
        "    # Convert the list of position encodings to a sparse matrix\n",
        "    pos_encodings_matrix = csr_matrix(encodings)\n",
        "\n",
        "    return pos_encodings_matrix"
      ],
      "metadata": {
        "id": "667b6Ctqccfn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sum_encodings_embeddings(X,Y):\n",
        "  return (X+Y).toarray()"
      ],
      "metadata": {
        "id": "UgfAP7jqBedQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xavier_init(fan_in, fan_out):\n",
        "    limit = np.sqrt(6 / (fan_in + fan_out))\n",
        "    return np.random.uniform(-limit, limit, size=(fan_in, fan_out))\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "def self_attention(X, model_size=50, d_k = 50):\n",
        "  Q, K, V = X, X, X\n",
        "  d_model = model_size\n",
        "  W_Q = xavier_init(d_model, d_k)\n",
        "  W_K = xavier_init(d_model, d_k)\n",
        "  W_V = xavier_init(d_model, d_k)\n",
        "  Q_i = np.dot(Q, W_Q)\n",
        "  K_i = np.dot(K, W_K)\n",
        "  V_i = np.dot(V, W_V)\n",
        "\n",
        "  attention_scores = np.dot(Q_i, K_i.T) / np.sqrt(d_k)\n",
        "  attention_weights = softmax(attention_scores)\n",
        "\n",
        "  # Compute the attention output\n",
        "  attention_output = np.dot(attention_weights, V_i)\n",
        "\n",
        "  return attention_output"
      ],
      "metadata": {
        "id": "ksrt1weCmm5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def layer_norm(x, eps=1e-6):\n",
        "    mean = np.mean(x, axis=-1, keepdims=True)\n",
        "    std = np.std(x, axis=-1, keepdims=True)\n",
        "    return (x - mean) / (std + eps)\n",
        "\n",
        "# Y is the result from the multihead attention\n",
        "def add_and_norm(X, Y):\n",
        "  return layer_norm(X + Y)"
      ],
      "metadata": {
        "id": "8-JAEg4b5XK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def feedforward_network(x, d_ff, d_model=50):\n",
        "    \"\"\"\n",
        "    Apply the feedforward network to the input x.\n",
        "\n",
        "    Parameters:\n",
        "    - x: Input array with shape (batch_size, sequence_length, d_model)\n",
        "    - d_model: Dimensionality of the model (input and output dimensions)\n",
        "    - d_ff: Dimensionality of the hidden layer in the feedforward network\n",
        "\n",
        "    Returns:\n",
        "    - Output array with the same shape as input x\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize weights and biases for the two layers\n",
        "    W1 = xavier_init(d_model, d_ff)\n",
        "    b1 = xavier_init(x.shape[0], d_ff)\n",
        "    W2 = xavier_init(d_ff, x.shape[1])\n",
        "    b2 = xavier_init(x.shape[0], x.shape[1])\n",
        "\n",
        "    # Apply the feedforward network\n",
        "    hidden_layer = np.dot(x, W1) + b1\n",
        "    hidden_layer = np.maximum(0, hidden_layer)  # ReLU activation\n",
        "    output_layer = np.dot(hidden_layer, W2) + b2\n",
        "\n",
        "    return output_layer"
      ],
      "metadata": {
        "id": "HfuBxNJqAEmA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load a pre-trained Word2Vec model\n",
        "word2vec_model = load('glove-wiki-gigaword-50')"
      ],
      "metadata": {
        "id": "rU-YrjqnatYg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def encoder(input, model):\n",
        "  #cleaned_input = clean_text(file_path, lemmatize = True, html_parser = False, rm_numbers = True)\n",
        "  input_embeddings = create_embeddings(input, model)\n",
        "  pos_encodings = positional_encodings(input, model_size=50)\n",
        "  input_with_pos = sum_encodings_embeddings(input_embeddings, pos_encodings)\n",
        "  print(input_with_pos.shape)\n",
        "  self_head_attention = self_attention(input_with_pos, model_size=50, d_k = 50)\n",
        "  print(self_head_attention.shape)\n",
        "  adding_and_norm_one = add_and_norm(input_with_pos, self_head_attention,)\n",
        "  print(adding_and_norm_one.shape)\n",
        "  ff_layer = feedforward_network(adding_and_norm_one, d_ff= 150, d_model=50)\n",
        "  print(ff_layer.shape)\n",
        "  adding_and_norm_two = add_and_norm(ff_layer, adding_and_norm_one)\n",
        "  return softmax(adding_and_norm_two)"
      ],
      "metadata": {
        "id": "dsNZJ0O0ARZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.datasets import imdb\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# # Load IMDB dataset\n",
        "# (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "\n",
        "# # Preprocess data: pad sequences to ensure uniform input size\n",
        "# maxlen = 100\n",
        "# train_data = pad_sequences(train_data, maxlen=maxlen)\n",
        "# test_data = pad_sequences(test_data, maxlen=maxlen)\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZPVE7XoFUr5",
        "outputId": "3a2478f9-52ad-43a2-8ebe-8df203de0cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Get the word index dictionary\n",
        "# word_index = imdb.get_word_index()\n",
        "\n",
        "# # Invert the word index to get a mapping from integer indices to words\n",
        "# reverse_word_index = {value: key for (key, value) in word_index.items()}\n",
        "\n",
        "# # Decode a review (let's take the first one from train_data)\n",
        "# decoded_review = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Colab Notebooks/SMSSpamCollection\", \"r\") as file:\n",
        "  file_content = file.readlines()\n",
        "spam_content = [line for line in file_content if \"spam\" in line]\n",
        "#print(spam_content[:2])\n",
        "sp_content = [line[5:] for line in spam_content]\n",
        "print(sp_content[:2])\n",
        "# clean_text(\"/content/drive/MyDrive/Colab Notebooks/SMSSpamCollection\", True , False, True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Nq7jM6ZONHM",
        "outputId": "a93ec296-a505-4d5c-f434-5bfe022f59d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\n\", \"FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, Â£1.50 to rcv\\n\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sp_content))\n",
        "train_data = sp_content[:498]\n",
        "test_data = sp_content[498:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikSTLG5lZ1Gu",
        "outputId": "38204f92-3a19-4af5-e2f8-10e00d2e995e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = train_data[0]\n",
        "print(sample_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHzOIUWDidrQ",
        "outputId": "fb0c1541-4bbe-4242-e2a9-21731b7732d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = [1 for elem in train_data]\n",
        "predicted_list = []\n",
        "for i in range(len(train_labels)):\n",
        "  print(train_data[i])\n",
        "  cleaned_text = clean_text(train_data[i], True, True, True)\n",
        "  print(cleaned_text)\n",
        "  result = encoder(cleaned_text, word2vec_model)\n",
        "  predicted_list.append(np.argmax(result))\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpHhdkpYYqmr",
        "outputId": "f2c70495-065a-49dd-8d5b-9acb228dd23f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\n",
            "\n",
            "['free', 'entry', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', 'st', 'may', '.', 'text', 'fa', 'receive', 'entry', 'question', '(', 'std', 'txt', 'rate', ')', '&', 'c', \"'s\", 'apply', \"'s\"]\n",
            "(27, 50)\n",
            "(27, 50)\n",
            "(27, 50)\n",
            "(27, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(predicted_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr0D4HFcrVp-",
        "outputId": "284cf46f-7b61-4145-b6d2-e6fac7b81973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[900]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = encoder(train_data, word2vec_model)\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Example: Using Logistic Regression for classification\n",
        "classifier = LogisticRegression()\n",
        "classifier.fit(result, train_labels)  # Ensure you use the correct labels here\n",
        "\n",
        "# Predict on test data\n",
        "test_result = encoder(test_data, word2vec_model)\n",
        "predictions = classifier.predict(test_result)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(test_labels, predictions)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "_j2ynYYIFr2s",
        "outputId": "3498002e-dbaa-4c92-bf5e-0231c3585715"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "unhashable type: 'numpy.ndarray'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-e0386f4a1e1e>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword2vec_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Example: Using Logistic Regression for classification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-6e90e5bffad0>\u001b[0m in \u001b[0;36mencoder\u001b[0;34m(input, model)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0;31m#cleaned_input = clean_text(file_path, lemmatize = True, html_parser = False, rm_numbers = True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m   \u001b[0minput_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m   \u001b[0mpositional_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpositional_encodings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0minput_with_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_encodings_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositional_encodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-4f3999f1eb30>\u001b[0m in \u001b[0;36mcreate_embeddings\u001b[0;34m(sent_tokens, model)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m# embed_matrix = csr_matrix((num_rows, num_columns))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msent_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#model.key_to_index:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m       \u001b[0mword_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__contains__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmost_similar_to_given\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeys_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mhas_index_for\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \"\"\"\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \"\"\"\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_to_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving model\n",
        "import joblib\n",
        "\n",
        "# Save the classifier\n",
        "joblib.dump(classifier, 'classifier_model.pkl')\n",
        "# Save the encoder if needed\n",
        "joblib.dump(encoder, 'encoder_model.pkl')"
      ],
      "metadata": {
        "id": "A0tmwrOuPjH9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading model\n",
        "# Load the classifier\n",
        "classifier = joblib.load('classifier_model.pkl')"
      ],
      "metadata": {
        "id": "Lr7z-QUJPluc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}