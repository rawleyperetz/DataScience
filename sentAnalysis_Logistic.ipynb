{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hozO5zSOQmQt",
        "outputId": "6790a9a1-1f79-4df0-bf5e-704d6a915371"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile"
      ],
      "metadata": {
        "id": "kYDBxbS9RCXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "file_path = \"/content/drive/MyDrive/Colab Notebooks/movie_archive.zip\"\n",
        "if os.path.isfile(file_path):\n",
        "    print(f\"File exists: {file_path}\")\n",
        "else:\n",
        "    print(f\"File does not exist: {file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8dUdMLeVT--",
        "outputId": "ccd1e7d6-788b-46ff-e866-8fde6428fdca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File exists: /content/drive/MyDrive/Colab Notebooks/movie_archive.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#zip_file_path =\n",
        "with zipfile.ZipFile(file_path) as zf:\n",
        "    directories = zf.namelist()\n",
        "    pos_dir = [subdir for subdir in directories if \"pos\" in subdir]\n",
        "    neg_dir = [subdir for subdir in directories if \"neg\" in subdir]\n",
        "\n",
        "        # if not file.endswith('.png'): # optional filtering by filetype\n",
        "        #     continue\n",
        "        # with zf.open(file) as f:\n",
        "        #     image = pygame.image.load(f, namehint=file)"
      ],
      "metadata": {
        "id": "A35aRhsaRfzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(pos_dir[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtTfBPQbY6MV",
        "outputId": "7803751b-660d-4de4-d44d-4f6d3af03db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "movie_reviews/movie_reviews/pos/cv000_29590.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_percent = 0.8\n",
        "pos_train_dir = pos_dir[:int(train_percent*len(pos_dir))]\n",
        "pos_test_dir = pos_dir[int(train_percent*len(pos_dir)):]\n",
        "\n",
        "neg_train_dir = neg_dir[:int(train_percent*len(neg_dir))]\n",
        "neg_test_dir = neg_dir[int(train_percent*len(neg_dir)):]"
      ],
      "metadata": {
        "id": "6FWaBNVaXrmh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Example multi-line string\n",
        "# multi_line_string = \"\"\"\n",
        "# <p>Hello, this is a sample text with <b>HTML</b> tags.</p>\n",
        "# <p>Here are some numbers: 123, 456, 789.</p>\n",
        "# <p>Another line with numbers and <a href=\"#\">more HTML</a> tags.</p>\n",
        "# \"\"\"\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(f_path):\n",
        "  with zipfile.ZipFile(\"/content/drive/MyDrive/Colab Notebooks/movie_archive.zip\", 'r') as zip_ref:\n",
        "    with zip_ref.open(f_path) as file:\n",
        "      file_content = file.read().decode('utf-8').lower()\n",
        "    #content = file_content.strip().split('\\n')\n",
        "    content = BeautifulSoup(file_content, \"html.parser\").text\n",
        "    content = re.sub(r'\\d+', '', content)\n",
        "    content = ' '.join(content.split())\n",
        "    tokens = word_tokenize(content)\n",
        "    tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "  return lemmatized_tokens\n",
        "\n",
        "\n",
        "# # 1. Split the string into lines\n",
        "# lines = multi_line_string.strip().split('\\n')\n",
        "\n",
        "# # Function to remove HTML tags\n",
        "# def remove_html_tags(text):\n",
        "#     return BeautifulSoup(text, \"html.parser\").text\n",
        "\n",
        "# # Function to remove numbers from a string\n",
        "# def remove_numbers(text):\n",
        "#     return re.sub(r'\\d+', '', text)\n",
        "\n",
        "# # Function to remove extra whitespaces\n",
        "# def remove_extra_whitespaces(text):\n",
        "#     return ' '.join(text.split())\n",
        "\n",
        "# # Process each line to remove numbers, HTML tags, and extra whitespaces\n",
        "# cleaned_lines = []\n",
        "# for line in lines:\n",
        "#     # Remove HTML tags\n",
        "#     line_without_html = remove_html_tags(line)\n",
        "#     # Remove numbers\n",
        "#     line_no_numbers = remove_numbers(line_without_html)\n",
        "#     # Remove extra whitespaces\n",
        "#     line_cleaned = remove_extra_whitespaces(line_no_numbers)\n",
        "#     cleaned_lines.append(line_cleaned)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoHOWKQwb5d-",
        "outputId": "45c87c61-6b9d-465b-cf38-e003ae8b4f57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from gensim.models import KeyedVectors\n",
        "# import gensim.downloader as api\n",
        "from gensim.downloader import load\n",
        "\n",
        "# Load a pre-trained Word2Vec model\n",
        "word2vec_model = load('glove-wiki-gigaword-50')\n",
        "# Load a pre-trained Word2Vec model from gensim's API\n",
        "#word2vec_model = api.load(\"word2vec-google-news-300\")\n",
        "# Load the pre-trained Word2Vec model\n",
        "# Replace 'path/to/GoogleNews-vectors-negative300.bin' with the actual path to your model file\n",
        "# model_path = 'GoogleNews-vectors-negative300.bin'\n",
        "# word2vec_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "RlGr54UdZbQq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(token, model):\n",
        "        if token in model:\n",
        "            return model[token]\n",
        "        else:\n",
        "          return np.zeros(model.vector_size)\n",
        "\n",
        "            # Handle out-of-vocabulary (OOV) words if needed\n",
        "\n",
        "# def get_embeddings(tokens, model):\n",
        "#     embeddings = {}\n",
        "#     for token in tokens:\n",
        "#         if token in model:\n",
        "#             embeddings[token] = model[token]\n",
        "#         else:\n",
        "#             # Handle out-of-vocabulary (OOV) words if needed\n",
        "#             continue #embeddings[token] = None\n",
        "#     return embeddings"
      ],
      "metadata": {
        "id": "y7hBuCfhj-2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from scipy.sparse import vstack\n",
        "from scipy.sparse import csr_matrix\n",
        "num_columns = 50\n",
        "num_rows = 0  # No rows initially\n",
        "\n",
        "# Create the csr_matrix\n",
        "embed_matrix = csr_matrix((num_rows, num_columns))\n",
        "\n",
        "for each_path in pos_train_dir:\n",
        "  # with open(each_path, 'r') as file:\n",
        "  #   text = file.read()\n",
        "    cleaned_text = clean_text(each_path)\n",
        "    num_of_tokens = len(cleaned_text)\n",
        "    if num_of_tokens == 0:\n",
        "      avg_file_embedding = np.zeros(num_columns)\n",
        "    else:\n",
        "      sum_embeddings = 0\n",
        "      for token in cleaned_text:\n",
        "        sum_embeddings += get_embeddings(token, word2vec_model)\n",
        "    avg_file_embedding = sum_embeddings / num_of_tokens\n",
        "    data_matrix = csr_matrix(avg_file_embedding).reshape(1,-1)\n",
        "\n",
        "# Append the new data to the existing matrix\n",
        "    embed_matrix = vstack([embed_matrix, data_matrix])\n",
        "\n",
        "\n",
        "for each_path in neg_train_dir:\n",
        "  # with open(each_path, 'r') as file:\n",
        "  #   text = file.read()\n",
        "    cleaned_text = clean_text(each_path)\n",
        "    num_of_tokens = len(cleaned_text)\n",
        "    if num_of_tokens == 0:\n",
        "      avg_file_embedding = np.zeros(num_columns)\n",
        "    else:\n",
        "      sum_embeddings = 0\n",
        "      for token in cleaned_text:\n",
        "        sum_embeddings += get_embeddings(token, word2vec_model)\n",
        "    avg_file_embedding = sum_embeddings / num_of_tokens\n",
        "    ndata_matrix = csr_matrix(avg_file_embedding).reshape(1,-1)\n",
        "\n",
        "# Append the new data to the existing matrix\n",
        "    embed_matrix = vstack([embed_matrix, ndata_matrix])\n"
      ],
      "metadata": {
        "id": "TLXW8D0-kJq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ones = np.ones(int(train_percent*len(pos_dir)))\n",
        "\n",
        "# Step 2: Create an array of 0s with 800 elements\n",
        "zeros = np.zeros(int(train_percent*len(pos_dir)))\n",
        "\n",
        "# Step 3: Concatenate the two arrays\n",
        "labels = np.concatenate((ones, zeros))"
      ],
      "metadata": {
        "id": "H0qwfDaWxFK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# Assuming `embed_matrix` is a csr_matrix and `labels` is a NumPy array of labels\n",
        "# embed_matrix = csr_matrix(...)  # Your CSR matrix of embeddings\n",
        "# labels = np.array(...)  # Your labels (0 or 1 for binary classification)\n",
        "\n",
        "# Initialize and train the logistic regression model\n",
        "model = LogisticRegression(max_iter=1000)  # Increase max_iter if convergence issues occur\n",
        "model.fit(embed_matrix, labels)\n",
        "\n",
        "# Make predictions\n",
        "# predictions = model.predict(embed_matrix)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "jaaBG1G9kmNa",
        "outputId": "6003e763-874d-452a-f1dd-dca50f57a010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=1000)"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=1000)</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nembed_matrix = csr_matrix((num_rows, num_columns))\n",
        "for each_path in neg_test_dir:\n",
        "  # with open(each_path, 'r') as file:\n",
        "  #   text = file.read()\n",
        "    cleaned_text = clean_text(each_path)\n",
        "    num_of_tokens = len(cleaned_text)\n",
        "    if num_of_tokens == 0:\n",
        "      avg_file_embedding = np.zeros(num_columns)\n",
        "    else:\n",
        "      sum_embeddings = 0\n",
        "      for token in cleaned_text:\n",
        "        sum_embeddings += get_embeddings(token, word2vec_model)\n",
        "    avg_file_embedding = sum_embeddings / num_of_tokens\n",
        "    ndata_matrix = csr_matrix(avg_file_embedding).reshape(1,-1)\n",
        "\n",
        "# Append the new data to the existing matrix\n",
        "    nembed_matrix = vstack([nembed_matrix, ndata_matrix])"
      ],
      "metadata": {
        "id": "J8VzLVduyfFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(nembed_matrix)"
      ],
      "metadata": {
        "id": "fZQg7DBfy9SO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Example list\n",
        "\n",
        "\n",
        "# Use Counter to count occurrences\n",
        "occurrences_dict = Counter(predictions)\n",
        "\n",
        "print(occurrences_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGzJqZBqzGjB",
        "outputId": "235a77f5-17af-459b-b859-2e933d749a34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counter({0.0: 149, 1.0: 51})\n"
          ]
        }
      ]
    }
  ]
}